<!DOCTYPE html>
<html>

  <head>
    <meta charset='utf-8'>
    <meta http-equiv="X-UA-Compatible" content="chrome=1">
    <meta name="description" content="Coursera pml : ">

    <link rel="stylesheet" type="text/css" media="screen" href="stylesheets/stylesheet.css">

    <title>Coursera pml</title>
  </head>

  <body>

    <!-- HEADER -->
    <div id="header_wrap" class="outer">
        <header class="inner">
          <a id="forkme_banner" href="https://github.com/swpark7/coursera_pml">View on GitHub</a>

          <h1 id="project_title">Coursera pml</h1>
          <h2 id="project_tagline"></h2>

            <section id="downloads">
              <a class="zip_download_link" href="https://github.com/swpark7/coursera_pml/zipball/master">Download this project as a .zip file</a>
              <a class="tar_download_link" href="https://github.com/swpark7/coursera_pml/tarball/master">Download this project as a tar.gz file</a>
            </section>
        </header>
    </div>

    <!-- MAIN CONTENT -->
    <div id="main_content_wrap" class="outer">
      <section id="main_content" class="inner">
        <p>&lt;!DOCTYPE html&gt;</p>

<p></p>

<p></p>

<p>

</p>

<p></p>

<p></p>Machine Learning for Human Activity Recognition



<p>

</p>



code{white-space: pre;}

<p></p>




  pre:not([class]) {
    background-color: white;
  }




<p></p>

<p></p>


.main-container {
  max-width: 940px;
  margin-left: auto;
  margin-right: auto;
}


<div>


<div id="header">
<h1>
<a id="machine-learning-for-human-activity-recognition" class="anchor" href="#machine-learning-for-human-activity-recognition" aria-hidden="true"><span class="octicon octicon-link"></span></a>Machine Learning for Human Activity Recognition</h1>
<h4>
<a id="steven-park" class="anchor" href="#steven-park" aria-hidden="true"><span class="octicon octicon-link"></span></a><em>Steven Park</em>
</h4>
<h4>
<a id="sunday-december-21-2014" class="anchor" href="#sunday-december-21-2014" aria-hidden="true"><span class="octicon octicon-link"></span></a><em>Sunday, December 21, 2014</em>
</h4>
</div>

<p>This is a document for a machine learning project with Human Activity Recognition. Human Activity Recognition - HAR - has emerged as a key research area in the last years and is gaining increasing attention by the pervasive computing research community (see picture below, that illustrates the increasing number of publications in HAR with wearable accelerometers), especially for the development of context-aware systems. There are many potential applications for HAR, like: elderly monitoring, life log systems for monitoring energy expenditure and for supporting weight-loss programs, and digital assistants for weight lifting exercises.</p>

<p>Details : <a href="http://groupware.les.inf.puc-rio.br/har">http://groupware.les.inf.puc-rio.br/har</a></p>

<div id="data-and-preperation">
<h3>
<a id="data-and-preperation" class="anchor" href="#data-and-preperation" aria-hidden="true"><span class="octicon octicon-link"></span></a>Data and preperation</h3>
<p>The training data for this project are available here. Pleaes download two files to your working directory.</p>
<pre><code>https://d396qusza40orc.cloudfront.net/predmachlearn/pml-training.csv</code></pre>
<p>The test data are available here:</p>
<pre><code>https://d396qusza40orc.cloudfront.net/predmachlearn/pml-testing.csv</code></pre>
<p>You need to install caret library first.</p>
<pre><code>install.packages("caret")
install.packages("rpart")
install.packages("gbm")
install.packages("randomForest")
install.packages("plyr")
install.packages("lattice")
install.packages("ggplot2")</code></pre>
<p>Load libraries</p>
<pre><code>library(caret) 
library(rpart) 
library(gbm)
library(randomForest) 
library(plyr)</code></pre>
<p>Read two downloaded files</p>
<pre><code>data &lt;- read.csv("pml-training.csv")
test &lt;- read.csv("pml-testing.csv")</code></pre>
<p>You need to remove columns having more than 10% of NA or empty. Also columns like row numbers, user names, timestamps should be removed.</p>
<pre><code>col &lt;- (colSums(is.na(data)) &lt; nrow(data) * 0.1) &amp; 
    (colSums(data=="") &lt; nrow(data) * 0.1) &amp; 
    !(colnames(data) %in% c("X", "user_name", "raw_timestamp_part_1", "raw_timestamp_part_2", "cvtd_timestamp"))
data1 &lt;- data[col]</code></pre>
<p>The data will be partitioned to training set (75%) and cross valication set (25%).</p>
<pre><code>inTrain &lt;- createDataPartition(y=data1$classe, p=0.75, list=F)
training &lt;- data1[inTrain,]
cv &lt;- data1[-inTrain,]
dim(training)</code></pre>
<pre><code>## [1] 14718    55</code></pre>
<p>We’ll test the three machine learning methods with caret package on training set and validate on cross valication set to compare the error rate. the best method will be choosed.</p>
<ul>
<li>Recursive Partitioning and Regression Trees</li>
<li>Generalized Boosted Regression Models</li>
</ul>
</div>

<div id="recursive-partitioning-and-regression-trees">
<h3>
<a id="recursive-partitioning-and-regression-trees" class="anchor" href="#recursive-partitioning-and-regression-trees" aria-hidden="true"><span class="octicon octicon-link"></span></a>Recursive Partitioning and Regression Trees</h3>
<p>Runing “Recursive Partitioning and Regression Trees” with caret package.</p>
<pre><code>set.seed(1995)
m_rpart &lt;- train(classe~., data=training, method="rpart")</code></pre>
<pre><code>## CART 
## 
## 14718 samples
##    54 predictor
##     5 classes: 'A', 'B', 'C', 'D', 'E' 
## 
## No pre-processing
## Resampling: Bootstrapped (25 reps) 
## 
## Summary of sample sizes: 14718, 14718, 14718, 14718, 14718, 14718, ... 
## 
## Resampling results across tuning parameters:
## 
##   cp       Accuracy  Kappa    Accuracy SD  Kappa SD
##   0.03812  0.5348    0.40075  0.04305      0.06805 
##   0.05542  0.4333    0.23708  0.07247      0.11691 
##   0.11829  0.3226    0.06021  0.04176      0.06403 
## 
## Accuracy was used to select the optimal model using  the largest value.
## The final value used for the model was cp = 0.03812.</code></pre>
<p>The error rate over cross valication samples</p>
<pre><code>p_rpart &lt;- predict(m_rpart, newdata=cv)
c_rpart &lt;- confusionMatrix(p_rpart, cv$classe)
e_rpart &lt;- as.numeric(1-c_rpart$overall[1])
e_rpart</code></pre>
<pre><code>## [1] 0.4843</code></pre>
<p>You can predict with test samples</p>
<pre><code>predict(m_rpart, newdata=test)</code></pre>
<pre><code>##  [1] C A C A A C C A A A C C C A C A A A A C
## Levels: A B C D E</code></pre>
</div>

<div id="generalized-boosted-regression-models">
<h3>
<a id="generalized-boosted-regression-models" class="anchor" href="#generalized-boosted-regression-models" aria-hidden="true"><span class="octicon octicon-link"></span></a>Generalized Boosted Regression Models</h3>
<p>Runing “Generalized Boosted Regression Models” with caret package.</p>
<pre><code>set.seed(1995)
m_gbm &lt;- train(classe~., data=training, method="gbm")</code></pre>
<pre><code>## Stochastic Gradient Boosting 
## 
## 14718 samples
##    54 predictor
##     5 classes: 'A', 'B', 'C', 'D', 'E' 
## 
## No pre-processing
## Resampling: Bootstrapped (25 reps) 
## 
## Summary of sample sizes: 14718, 14718, 14718, 14718, 14718, 14718, ... 
## 
## Resampling results across tuning parameters:
## 
##   interaction.depth  n.trees  Accuracy  Kappa   Accuracy SD  Kappa SD
##   1                   50      0.7609    0.6968  0.006583     0.008218
##   1                  100      0.8281    0.7825  0.005495     0.006872
##   1                  150      0.8675    0.8324  0.005511     0.006893
##   2                   50      0.8837    0.8528  0.006057     0.007618
##   2                  100      0.9385    0.9222  0.004594     0.005811
##   2                  150      0.9609    0.9505  0.003459     0.004381
##   3                   50      0.9312    0.9130  0.005672     0.007189
##   3                  100      0.9689    0.9607  0.003251     0.004120
##   3                  150      0.9838    0.9796  0.002693     0.003402
## 
## Tuning parameter 'shrinkage' was held constant at a value of 0.1
## Accuracy was used to select the optimal model using  the largest value.
## The final values used for the model were n.trees = 150,
##  interaction.depth = 3 and shrinkage = 0.1.</code></pre>
<p>The error rate over cross valication samples</p>
<pre><code>p_gbm &lt;- predict(m_gbm, newdata=cv)
c_gbm &lt;- confusionMatrix(p_gbm, cv$classe)
e_gbm &lt;- as.numeric(1-c_gbm$overall[1])
e_gbm</code></pre>
<pre><code>## [1] 0.01203</code></pre>
<p>You can predict with test samples</p>
<pre><code>predict(m_gbm, newdata=test)</code></pre>
<pre><code>##  [1] B A B A A E D B A A B C B A E E A B B B
## Levels: A B C D E</code></pre>
</div>

<div id="comparisons-with-error-rates-over-validation-set">
<h3>
<a id="comparisons-with-error-rates-over-validation-set" class="anchor" href="#comparisons-with-error-rates-over-validation-set" aria-hidden="true"><span class="octicon octicon-link"></span></a>Comparisons with error rates over validation set</h3>
<p>Generalized Boosted Regression Models is better with the following error rates</p>
<pre><code>e_rpart # Error rate of Recursive Partitioning and Regression Trees</code></pre>
<pre><code>## [1] 0.4843</code></pre>
<pre><code>e_gbm # Error of Generalized Boosted Regression Models</code></pre>
<pre><code>## [1] 0.01203</code></pre>
</div>

<p></p>
</div>







<p>
</p>
      </section>
    </div>

    <!-- FOOTER  -->
    <div id="footer_wrap" class="outer">
      <footer class="inner">
        <p class="copyright">Coursera pml maintained by <a href="https://github.com/swpark7">swpark7</a></p>
        <p>Published with <a href="http://pages.github.com">GitHub Pages</a></p>
      </footer>
    </div>

    

  </body>
</html>
